---
layout: post
title: Milestone 2
---

# Milestone 2 Presented by [this notebook](https://github.com/Jawing/ift6758-project-template-main/blob/main/milestone2.ipynb)

## Experiment Tracking

## Feature Engineering I

### Q2.1

<div class="message">
Create and include the following figures in your blog post and briefly discuss your observations (few sentences). As always, make sure all of your axes are labeled correctly, and you make the appropriate choice of axis scale.:

</div>

- A histogram of shot counts (goals and no-goals separated), binned by *angle*

Although teams like to shoot around -30, 0 (meaning shooting directly in front of the net) and 30 degrees, the goals are distributed as bell-curve shape with 0 degree shots having the highest probability to score.

<img src = "/assets/images/milestone2/m2_q2_1_angle_to_goal.png">

- A histogram of shot counts (goals and no-goals separated), binned by *distance*

Scoring is harder when the shot distance is larger than 25 feet, yet teams shots are quite evenly distributed across different distances.
Shots are rare past 70 feet, which makes sense as it is shot from the defensive zone.

<img src = "/assets/images/milestone2/m2_q2_1_dist_to_goal.png">

- A 2D histogram where one axis is the distance and the other is the angle. You do not need to separate goals and no-goals.
Hint: check out joint plots.

It seems that shots have a very hard time turning into goals when shot angles are larger than 50 or when distance larger than 40.

<img src = "/assets/images/milestone2/m2_q2_1_joint_ang_to_dist.png">

Smoothed by kde:
<img src = "/assets/images/milestone2/m2_q2_1_joint_ang_to_dist_kde.png">

### Q2.2

<div class="message">
Now, create two more figures relating the goal rate, i.e., #goals / (#no_goals + #goals), to the distance, and goal rate to the angle of the shot. Include these figures in your blog post and briefly discuss your observations.
</div>

The goal ratio is highest within 20 feet, lower when between 20-40 feet and much lower outside of 40 feet. It seems that as shot distance increase past 100 feet, the goal ratio starts to increase again. It suggest that, although the number of shots taken at a higher distance is low, the opportunity of scoring when taking a long shot is high. This makes sense especially when there is an empty net.

Distance:
<img src = "/assets/images/milestone2/m2_q2_2_shot_dist_goal_ratio.png">

The goal ratio to shot angle seems to have a bell-shape curve around zero from 90 to -90 degrees. Although interestingly the goal ratio increases dramatically from -90 to -180 degrees. After looking at the data and verifying with some video examples ([Sharp Angle Shot footage](https://www.nhl.com/video/armia-scores-from-sharp-angle/t-300137966/c-67423703)), it suggest that, although the number of shots taken at this angle is low, the opportunity of scoring when taken is high.

Shot angle:
<img src = "/assets/images/milestone2/m2_q2_2_shot_angle_goal_ratio.png">

### Q2.3

<div class="message">
We can use our “domain knowledge” for some quick sanity checks! The domain knowledge is that “it is incredibly rare to score a non-empty net goal on the opposing team from within your defensive zone”. Knowing this, create another histogram, this time of goals only, binned by distance, and separate empty net and non-empty net events. Include this figure in your blog post and discuss your observations. Can you find any events that are incorrectly labeled? If yes, prove that one event is incorrectly labeled.
Hint: the NHL game center usually has video clips of goals for every game.
</div>

<img src = "/assets/images/milestone2/m2_q2_3_dist_to_emptynet.png">

Examples of wrong information (rinkSide):
Given gameID: 2018020953 and eventidx: 368 with [Jakub Voracek's goal](https://www.nhl.com/video/voraceks-late-game-tying-goal/t-300064528/c-66170503), we have found that data was wrongly recorded with a much larger shot distance while scoring on a non-empty net shown [here](https://www.nhl.com/gamecenter/pit-vs-phi/2019/02/23/2018020953#game=2018020953,game_state=final,game_tab=plays). Below are the images of the shot and rinkSide rotations.
<img src = "/assets/images/milestone2/wrongrnkside2.png">
<img src = "/assets/images/milestone2/rinkide3.png">
<img src = "/assets/images/milestone2/rinkside4.png">
<img src = "/assets/images/milestone2/rinkside5.png">

## Baseline Models

### Q3.1

<div class="message">
Using only the distance feature, train a Logistic Regression classifier with the completely default settings

Evaluate the accuracy (i.e., correctly predicted / total) of your model on the validation set. What do you notice? Look at the predictions and discuss your findings. What could be a potential issue? Include these discussions in your blog post.
</div>

A logistic regression model with only a distance to goal metric has an **accuracy** of 0.91 and an **AUC** score of 0.69. However, it pushes all its predictions towards non-goal situations, which means that will predicts all outcome to be a non-goal. Therefore, it has a zero score on precision, recall and f1-score for shots that are goals. This is a big issue because we want our model to predict situations that would result in a goal.

### Q3.2

<div class="message">
Based on your findings in Q1, we should explore other ways of evaluating our model. The first thing to note is that we are not actually interested in the binary prediction of whether a shot is a goal or not - we are interested in the probability that the model assigns to it. Scikit-learn provides this to you via the predict_proba(...) method; make sure you take the probabilities of the class that you care about! 

You will produce four figures (one curve per model per plot) to probe our model’s performance. Make sure you are using the probabilities obtained on the validation set:

1, Receiver Operating Characteristic (ROC) curves and the AUC metric of the ROC curve. Include a random classifier baseline, i.e., each shot has a 50% chance of being a goal.

2, The goal rate (#goals / (#no_goals + #goals)) as a function of the shot probability model percentile, i.e., if a value is the 70th percentile, it is above 70% of the data. 

3, The cumulative proportion of goals (not shots) as a function of the shot probability model percentile.

4, The reliability diagram (calibration curve). Scikit-learn provides functionality to create a reliability diagram in a few lines of code; check out the CalibrationDisplay API (specifically the .from_estimator() or .from_predictions() methods) for more information.

</div>
Below are the plots for each model.

**Model: with distance only**

- The cumulative proportion of goals (not shots) as a function of the shot probability model percentile.
<img src = "/assets/images/milestone2/q31_logR_CP.png">

- The goal rate (#goals / (#no_goals + #goals)) as a function of the shot probability model percentile. 
<img src = "/assets/images/milestone2/q31_logR_GR.png">

- Receiver Operating Characteristic (ROC) curves and the AUC metric of the ROC curve: The ROC curve value is 0.69
<img src = "/assets/images/milestone2/q31_logR_ROC_distance.png">

### Q3.3

<div class="message">
Now train two more Logistic Regression classifiers using the same setup as above, but this time on the angle feature, and then both distance and angle. 

Produce the same three curves as described in the previous section for each model.

Including the random baseline, you should have a total of 4 lines on each figure:

- Logistic Regression, trained on distance only (already done above)
- Logistic Regression, trained on angle only
- Logistic Regression, trained on both distance and angle
- Random baseline: predicted probability is sampled from a uniform distribution.

Include these four figures (each with four curves) in your blog post. In a few sentences, discuss your interpretation of these results.
</div>

**Model: with angle only**

<img src = "/assets/images/milestone2/q32_logR_CP.png">
<img src = "/assets/images/milestone2/q32_logR_GR.png">
<img src = "/assets/images/milestone2/q32_logR_ROC_angle.png">

**Model: with angle and distance**
<img src = "/assets/images/milestone2/q33_logR_CP.png">
<img src = "/assets/images/milestone2/q33_logR_GR.png">
<img src = "/assets/images/milestone2/q33_logR_ROC_angle_distance.png">

**Model: random**
<img src = "/assets/images/milestone2/q34_Random_CP.png">
<img src = "/assets/images/milestone2/q34_Random_GR.png">
<img src = "/assets/images/milestone2/q34_Random.png">

**Summary of all four models in one plot**

*Goal Rate*
<img src = "/assets/images/milestone2/q35_Merging_GR.png">

The goal rate curves of different models.
The logistic regression with angle only plot seems to be predicting a constant goal rate at every shot percentile, which does not seem to be reliable when determining goals. 

*Cumulative*
<img src = "/assets/images/milestone2/q35_Merging_CP.png">

The cumulative curves of different models.

*ROC Curves*
<img src = "/assets/images/milestone2/q35_Merging_ROC.png">

The ROC curves show that logistic regression with both distance and angle (or with just the angle) as features produce the best results. Logistic regression with only angle as feature produces a result not much better than a random guess.  

*Calibration Curves*
<img src = "/assets/images/milestone2/q35_Merging_calibration.png">

For the calibration curves, we can also see that logistic regression with angle and distance produces some improvement, but its probabilistic predictions hardly fits a perfectly calibrated binary classifier.

### Q3.4

<div class="message">
Q3.4 Next to the figures (above), include links to the three experiment entries in your comet.ml projects that produced these three models. Save the three models to the three experiments on comet.ml (example here) and register them with some informative tags, as you will need it for the final section. \
</div>

[Logistic Regression, trained on distance](https://www.comet.ml/binulal/milestone-2/4d23e3c07b2c4130a7f6dbb50b5eb1e7)

[Logistic Regression, trained on angle only](https://www.comet.ml/binulal/milestone-2/d909638434b943e79be423d292ec5378)

[Logistic regression , trained on both](https://www.comet.ml/binulal/milestone-2/57eb37f26f374217a1bf19e460ff0ce9)

## Feature Engineering II

### Q4.1
<div class="message">
 Upload the filtered DataFrame with all of the features that you created as a CSV using the log_dataframe_profile(...) method; keep the name as 'wpg_v_wsh_2017021065'.
</div>

The dataframe can be found in this [link](https://www.comet.ml/binulal/milestone-2/b95f15e394374ac0b4509d170efe357d?assetId=995878f6b20d469d8fc8d2eb65ebd2f8&assetPath=dataframes&experiment-tab=assets).

### Q4.2
<div class="message">
In your blog post, add a list of all of the features that you created for this section. 

List each feature by both the column names in your data frame AND a simple human-readable explanation (i.e., game_sec: Total number of seconds elapsed in the game). 

If you created any novel features, briefly describe what they are. 

Add a link to the experiment which stores the filtered DataFrame artifact for the specified game. Note that there should only be one DataFrame logged with this name.
</div>

**Feature list** (before preprocessing)

- Game_id : Id for the hockey game.
- Event_idx: Id for an event (shot or goal) in the game.
- Speed: (distance from the previous event)/(time since previous event seconds).
- periodSeconds_last: Time from the last event (seconds).
- eventType_last: Last event type.
- Rebound: True only if the last event was also a shot.
- Period: Period in the hockey game.
- periodType: Type of period in the game. ('REGULAR','OVERTIME','SHOOTOUT')
- periodTime: Min:sec, Time during the period.
- periodSeconds: PeriodTime converted to seconds.
- teamInfo: Name of team who made the shot.
- isGoal: True only if shot is goal.
- shotType: Different shot types.
- Coordinates_x: X coordinate where the event happened.
- Coordinates_y: Y coordinate where the event happened.
- Coordinates_x_last: X coordinate where the last event happened.
- Coordinates_y_last: Y coordinate where the last event happened.
- Distance_last: Distance between current and last event.
- Dist_goal: Distance between event and goal.
- Angle_goal: Angle [-180,180] between the shot event and goal.
- Angle_change: Only when the shot is a rebound, the change in angle between shots.
- Angle_speed: Angle_change/(time between current and last shot)
- Shooter: Name of the shooter.
- Goalie: Name of the goalie.
- emptyNet: True if a shot is taken on an empty Net.
- Strength: Strength of team on the playing field.
- homeTeam: Name of home team.
- awayTeam: Name of away team.
- homeSide: Left if home starts in period 1 on the left side, right otherwise.

Again the dataframe can be found in this [link](https://www.comet.ml/binulal/milestone-2/b95f15e394374ac0b4509d170efe357d?assetId=995878f6b20d469d8fc8d2eb65ebd2f8&assetPath=dataframes&experiment-tab=assets).

## Advanced Models

### Q5.1

<div class="message">
Add the corresponding curves to the four figures in your blog post. Briefly (few sentences) discuss your training/validation setup, and compare the results to the Logistic Regression baseline. Include a link to the relevant comet.ml entry for this experiment, but you do not need to log this model to the model registry.
</div>

We binarized the variable `isEmpty` and then use a simple train-validation split that uses one third of the data as validation cases.

The corresponding result is:
<img src="/assets/images/milestone2/q51_score.png">

Compared to the logistic regression baselinee:
<img src="/assets/images/milestone2/q31_score.png">

We can see that the XGboost model has already solve the issue that we have encountered in the Q3: we have some predictions for class 1 now.

The corresponding curves for XGboost model are displayed below.
<img src="/assets/images/milestone2/q51_XGboost_CP.png">

<img src="/assets/images/milestone2/q51_XGboost_GR.png">

<img src="/assets/images/milestone2/q51_Xgboost_ROC_distance_angle.png">

The ROC curve has AUC value of 0.71, which is better than the baseline logistic model, but not by much.

The [link to the model](https://www.comet.ml/binulal/milestone-2/8b7c8a744b18434f9a42bc454573f40d)

### Q5.2

<div class="message">
Now, train an XGBoost classifier using all of the features you created in Part 4 and do some hyperparameter tuning to try to find the best performing model with all of these features. In your blog post, discuss your hyperparameter tuning setup, and include figures to substantiate your choice of hyperparameters. For example, you could select appropriate metrics and do a grid search with cross validation. Once, include curves corresponding to the best model to the four figures in your blog post, and briefly compare the results to the XGBoost baseline. Include a link to the relevant comet.ml entry for this experiment, and log this model to the model registry.
</div>

**Hyperparameter tune up**
We have tried **Grid search** and **randomized search**. Since grid search took a long time to run (in our case, 11 hours), we mainly rely on thee randomized search for the result. Cross-validation is strategy is a 3-fold stratified sample.

The optimal choice, according to our randomized search, is documented below:
<img src="/assets/images/milestone2/q53_score.png">

The corresponding result is:
<img src="/assets/images/milestone2/q52_score.png">

There is a substantial improvement over the previous model. Most importantly, the precision for class 1 prediction has risen from 0.4 to 0.59.

The corresponding curves for XGboost model with all features are displayed below:

<img src="/assets/images/milestone2/q52_XGboost_CP.png">

<img src="/assets/images/milestone2/q52_XGboost_GR.png">

<img src="/assets/images/milestone2/q52_Xgboost_ROC.png">

The [link to the comet.ml](https://www.comet.ml/binulal/milestone-2/8d75cbfc7cf9446f90560acf7c51ea53)

### Q5.3

<div class="message">
Finally, explore using some feature selection techniques to see if you can simplify your input features. A number of features carry correlated information, so you can try to see if some of them are redundant. You can try some of the feature selection techniques discussed in class; many of these are implemented for you by scikit-learn. You could also use a library like SHAP to try to interpret what feature your model relies on the most. Discuss the feature selection strategies that you tried, and what was the most optimal set of features that you came up with. Include some figures to substantiate your claims. Once you’ve found the optimal set of features via hyperparameter tuning/cross validation, if the feature set is different than what was used for Q2 of this section, include curves corresponding to the best model to the four figures in your blog post, and briefly compare the results to the XGBoost baseline. Include a link to the relevant comet.ml entry for this experiment, and log this model to the model registry.
</div>

We have used various feature selection methods such as correlation matrix, XGboost feature importance weight, chi square method and SHAP value.

The selected features are the top 10 most important features according to the picture listed below:
<img src="/assets/images/milestone2/q53_feature_1.png">

The SHAP value importance are as follows:
<img src="/assets/images/milestone2/q53_shap1.png">
<img src="/assets/images/milestone2/q53_shap2.png">
<img src="/assets/images/milestone2/q53_shap3.png">

The result is the following:
<img src="/assets/images/milestone2/q53_score.png">
Compared to the full model, there is a slight decrease of recall value and f1-score for the class 1 predictions. However, the precision has not dropped and remains at 0.59 for class 1 predictions.

The corresponding curves for XGboost model with selected features are displayed below:
<img src="/assets/images/milestone2/q53_Xgboost_ROC.png">
<img src="/assets/images/milestone2/q53_XGboost_tuned_CP.png">
<img src="/assets/images/milestone2/q53_XGboost_tuned_GR.png">

The [link to the comet.ml](https://www.comet.ml/binulal/milestone-2/2e98573c75804e6998136ba84d928b79)

## Give it your best shot

### Q6.1
<div class="message">
In your blog post, discuss the various techniques and methods you tried. Include the same four figures as in Part 3 (ROC/AUC curve, goal rate vs. probability percentile, cumulative proportion of goals vs. probability percentile, and the reliability curve). Quantitative metrics are only required for a few sets of experiments, so you only need to include a few curves on each plot (e.g., things that you found interesting, or models that performed particularly well). Make sure to include and highlight what you consider your best ‘final’ model. For methods that weren’t too successful or interesting, you can just include them as short qualitative discussion.
</div>

To come up with the best model, we made sure to preprocess the data into a numerical form.

- Redundant columns are dropped such as ‘game_id’, ‘event_idx’, ‘periodTime’.
- All columns with more than 60% NAN values are dropped.
- All rows with NAN values are dropped.
- ‘isGoal’, ‘rebound’, ‘emptyNet’, ‘homeSide’ columns are binarized into 0, 1.
- One hot encoding is applied to the columns ‘periodType’, ‘eventType_last’, ‘teamInfo’, ‘shotType’, ‘homeTeam’, ‘awayTeam’.
- ‘shooter’ and ‘goalie’ columns are dropped because of the low data variance. The total number of features ended up being 128.

After preprocessing, the data was **stratified-split** into train and validation sets.

For the **logistic regression model**, a **grid search** cross validation strategy was used to select the *C* parameter and the *number of components* after fitting the data with PCA. The data is first standardized in the pipeline, then PCA is applied then finally the logistic regression model. The resulting *best* parameters were PCA_n=90, C=10000.

Below are the plots for the logistic regression model with the same four types of figures from Q3.
<!-- <img src="/assets/images/milestone2/q61_logR_GR.png">
<img src="/assets/images/milestone2/q61_logR_CP.png">
<img src="/assets/images/milestone2/q61_logR_ROC.png">
<img src="/assets/images/milestone2/q61_logR_RD.png"> -->
- | - 
| ![alt](/assets/images/milestone2/q61_logR_GR.png) | ![alt](/assets/images/milestone2/q61_logR_CP.png) |
| ![alt](/assets/images/milestone2/q61_logR_ROC.png) | ![alt](/assets/images/milestone2/q61_logR_RD.png) |

For the **svc model**, a **grid search** cross validation strategy was used to select the *C, kernel* parameters. The data is first standardized in the pipeline, then PCA (with number of components = 85) is applied then finally the svc model. The resulting best parameters were C=10, kernel=’rbf’.

For the **random forest model**, The data is directly applied to the random forest model with a predefined parameter. The random forest model has an **AUC of 0.76**, highest among all models.

For the **adaboost model**, a grid search cross validation strategy was used to select the *n_estimators*, *learning_rate* parameters. The data is directly applied to the adaboost model. The *resulting* best parameters were n_estimators=10, learning_rate=0.01. The adaboost model has an **AUC of 0.68**.

Below are the plots for the adaboost model.
<!-- <img src="/assets/images/milestone2/q64_ada_GR.png">
<img src="/assets/images/milestone2/q64_ada_CP.png">
<img src="/assets/images/milestone2/q64_ada_ROC.png">
<img src="/assets/images/milestone2/q64_ada_RD.png"> -->
- | - 
| ![alt](/assets/images/milestone2/q64_ada_GR.png) | ![alt](/assets/images/milestone2/q64_ada_CP.png) |
| ![alt](/assets/images/milestone2/q64_ada_ROC.png) | ![alt](/assets/images/milestone2/q64_ada_RD.png) |

For the **mlp model**, a **grid search** cross validation strategy was used to select the *hidden_layer_sizes*. The data is first standardized in the pipeline, then PCA is applied then finally the mlp model. The resulting best parameters were C=10, kernel=’rbf’, hidden_layer_sizes = (100,100).

The MLP model seems to have a smooth curve on the shot prob model percentile plots. It has an **AUC of 0.74**

Below are the plots for the adaboost model.
<!-- <img src="/assets/images/milestone2/q65_mlp_GR.png">
<img src="/assets/images/milestone2/q65_mlp_CP.png">
<img src="/assets/images/milestone2/q65_mlp_ROC.png">
<img src="/assets/images/milestone2/q65_mlp_RD.png"> -->
- | -
| ![alt](/assets/images/milestone2/q65_mlp_GR.png) | ![alt](/assets/images/milestone2/q65_mlp_CP.png) |
| ![alt](/assets/images/milestone2/q65_mlp_ROC.png) | ![alt](/assets/images/milestone2/q65_mlp_RD.png) |

Finally For the **voting ensemble model**, we have included the previous random forest, mlp, logistic regression and adaboost models with *equal* weights. The data is fitted with *soft voting*. The final model used on the test set was the ensemble model in hopes for a better generalization performance. It has an **AUC of 0.76** as good as the random forest model.

Below are the plots for the voting ensemble model.
<!-- <img src="/assets/images/milestone2/q66_ens_GR.png">
<img src="/assets/images/milestone2/q66_ens_CP.png">
<img src="/assets/images/milestone2/q66_ens_ROC.png">
<img src="/assets/images/milestone2/q66_ens_RD.png"> -->
- | -
| ![alt](/assets/images/milestone2/q66_ens_GR.png) | ![alt](/assets/images/milestone2/q66_ens_CP.png) |
| ![alt](/assets/images/milestone2/q66_ens_ROC.png) | ![alt](/assets/images/milestone2/q66_ens_RD.png) |

The **precision** metric for most of the models are *close to 1, other than svc (0.6)*. The **f1 score** is *around 0.09* for most of the models, other than svc (0.12). The **recall score** is around *0.05* for most of the models, other than svc (0.067). These scores seem to be a lot lower because of the false negatives. It seems to be difficult for the models to learn which shots lead to a goal.

### Q6.2

<div class="message">
Next to the figures, include links to the experiment entry in your comet.ml projects that you included quantitative metrics for (around 3-4). Log the models to the experiments on comet.ml (example here) and register them with some informative tags.
</div>

Experiments for each of the models described above.
- [Q61_logR](https://www.comet.ml/binulal/milestone-2/dfed4cec3eb74e8282128c6b6f1d6beb)
- [Q62_svc](https://www.comet.ml/binulal/milestone-2/88d76b3e5bb046768dda45bbcf2b5fab)
- [Q63_rf](https://www.comet.ml/binulal/milestone-2/e2bec5ea1f8e458bb3d12c9c31536ced)  
- [Q64_ada](https://www.comet.ml/binulal/milestone-2/a4b50b1c75e94d2eb2d2fa53406b2040)
- [Q65_nn](https://www.comet.ml/binulal/milestone-2/87bd90b3aad74d48b1ec1da8374a4508)
- [Q66_ens](https://www.comet.ml/binulal/milestone-2/30164fae30564a85ae8dd1d0afe43dd7)

## Evaluate on test set

### Q7.1

<div class="message">
Test your 5 models on the untouched 2019/20 regular season dataset. In your blog post, include the four figures described above. Discuss your results and observations on the test set. Do your models perform as well on the test set as you did on your validation set when building your models? Do any models perform better or worse than you expected?
</div>

For models tested on the regular season test set, the results are the following:

<img src="/assets/images/milestone2/q7_calibration_Regular_test.png">
<img src="/assets/images/milestone2/q7_cumulative_Regular_test.png">
<img src="/assets/images/milestone2/q7_ROC_Regular_test.png">
<img src="/assets/images/milestone2/q7_goalrate_Regular_test.png">

The voting [ensemble model](https://www.comet.ml/binulal/milestone-2/49e4d10bdb9b47ec99aef56ab5586c34) and the XGboost model had similar performance on the test set from the regular seasons as on the validation set. It performed slightly better on the test set for regular seasons. The **AUC score** of the ensemble and XGboost models are **0.76** and **0.77** respectively , on par with our result on the training data. This shows that the models gives a satisfactory result in terms of generalization.

### Q7.2

<div class="message">
Test your 5 models on the untouched 2019/20 playoff games. In your blog post, include the four figures described above. Discuss your results and observations on this test set. Are there any differences to the regular-season test set or do you get similar ‘generalization’ performance?
</div>

For models tested on the playoffs season test set, the results are the following:

<img src="/assets/images/milestone2/q7_calibration_Playoff_test.png">
<img src="/assets/images/milestone2/q7_cumulative_Playoff_test.png">
<img src="/assets/images/milestone2/q7_ROC_Playoff_test.png">
<img src="/assets/images/milestone2/q7_goalrate_Playoff_test.png">

The voting [ensemble model](https://www.comet.ml/binulal/milestone-2/d38380c1fd564ed78806e14674d74282)
seems to generalize well as its performance on the test set from the playoffs data is comparably the same as on the validation set from the regular season data. The **AUC score** of the ensemble method is **0.70**, which is a slight drop compared to our regular season prediction. It is still a satisfactory performance. However The fined tuned XGboost model had a better ROC curve with an **AUC score of 0.74** which seems to do a better job compared to the ensemble model at ‘generalization’ performance.

<!-- <img src="/assets/images/milestone2/q7m5_ens_po_CP.png">
<img src="/assets/images/milestone2/q7m5_ens_po_GR.png">
<img src="/assets/images/milestone2/q7m5_ens_po_RD.png">
<img src="/assets/images/milestone2/q7m5_ens_po_ROC.png"> -->
