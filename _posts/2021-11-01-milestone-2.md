---
layout: post
title: Milestone 2
---

# Milestone 2 Presented by [this notebook](https://github.com/Jawing/ift6758-project-template-main/blob/main/milestone2.ipynb)

## Experiment Tracking

## Feature Engineering I

### Q2.1 

<div class="message">
Create and include the following figures in your blog post and briefly discuss your observations (few sentences). As always, make sure all of your axes are labeled correctly, and you make the appropriate choice of axis scale.:
 
</div>

- A histogram of shot counts (goals and no-goals separated), binned by *angle*

Although teams like to shot at -30, 0 (meaning shooting directly in front of the net) and 30 degrees, the goals are distributed as bell-curve shape with 0 degree shots having the highest probability to score. 

<img src = "/assets/images/milestone2/m2_q2_1_angle_to_goal.png">

- A histogram of shot counts (goals and no-goals separated), binned by *distance*

Scoring is harder when shot distance is larger than 25 feet, yet teams shots are quite evenly distributed across different distances.

<img src = "/assets/images/milestone2/m2_q2_1_dist_to_goal.png">

- A 2D histogram where one axis is the distance and the other is the angle. You do not need to separate goals and no-goals.
Hint: check out joint plots.

It seems that shots have a very hard time turning into goals when shot angles are larger than 50 or when distance larger than 40.

<img src = "/assets/images/milestone2/m2_q2_1_joint_ang_to_dist.png">

Smoothed by kde:
<img src = "/assets/images/milestone2/m2_q2_1_joint_ang_to_dist_kde.png">



### Q2.2

<div class="message">
Now, create two more figures relating the goal rate, i.e., #goals / (#no_goals + #goals), to the distance, and goal rate to the angle of the shot. Include these figures in your blog post and briefly discuss your observations.
</div>

The goal ratio is highest within 20 feet, lower when between 20-40 feet and much lower outside of 40 feet.

Distance:
<img src = "/assets/images/milestone2/m2_q2_2_shot_dist_goal_ratio.png">

The goal ratio is a bell-shape curve with highest point at around zero.

Shot angle:
<img src = "/assets/images/milestone2/m2_q2_2_shot_angle_goal_ratio.png">

### Q2.3

<div class="message">
We can use our “domain knowledge” for some quick sanity checks! The domain knowledge is that “it is incredibly rare to score a non-empty net goal on the opposing team from within your defensive zone”. Knowing this, create another histogram, this time of goals only, binned by distance, and separate empty net and non-empty net events. Include this figure in your blog post and discuss your observations. Can you find any events that are incorrectly labeled? If yes, prove that one event is incorrectly labeled.
Hint: the NHL game center usually has video clips of goals for every game.
</div>

<img src = "/assets/images/milestone2/m2_q2_3_dist_to_emptynet.png">

Examples of wrong information:
<img src = "/assets/images/milestone2/wrongrnkside2.png">
<img src = "/assets/images/milestone2/rinkide3.png">
<img src = "/assets/images/milestone2/rinkside4.png">
<img src = "/assets/images/milestone2/rinkside5.png">



## Baseline Models

### Q3.1
<div class="message">
Using only the distance feature, train a Logistic Regression classifier with the completely default settings

Evaluate the accuracy (i.e., correctly predicted / total) of your model on the validation set. What do you notice? Look at the predictions and discuss your findings. What could be a potential issue? Include these discussions in your blog post.
</div>

<img src = "/assets/images/milestone2/q31_logR_CP.png">
<img src = "/assets/images/milestone2/q31_logR_GR.png">
<img src = "/assets/images/milestone2/q31_logR_ROC_distance.png">

### Q3.2

<div class="message">
Based on your findings in Q1, we should explore other ways of evaluating our model. The first thing to note is that we are not actually interested in the binary prediction of whether a shot is a goal or not - we are interested in the probability that the model assigns to it. Scikit-learn provides this to you via the predict_proba(...) method; make sure you take the probabilities of the class that you care about! You will produce four figures (one curve per model per plot) to probe our model’s performance. Make sure you are using the probabilities obtained on the validation set:
Receiver Operating Characteristic (ROC) curves and the AUC metric of the ROC curve. Include a random classifier baseline, i.e., each shot has a 50% chance of being a goal.
The goal rate (#goals / (#no_goals + #goals)) as a function of the shot probability model percentile, i.e., if a value is the 70th percentile, it is above 70% of the data. 
The cumulative proportion of goals (not shots) as a function of the shot probability model percentile.
The reliability diagram (calibration curve). Scikit-learn provides functionality to create a reliability diagram in a few lines of code; check out the CalibrationDisplay API (specifically the .from_estimator() or .from_predictions() methods) for more information.

</div>

The goal rate (#goals / (#no_goals + #goals)) as a function of the shot probability model percentile. 
<img src = "/assets/images/milestone2/q32_logR_CP.png">

The cumulative proportion of goals (not shots) as a function of the shot probability model percentile.
<img src = "/assets/images/milestone2/q32_logR_GR.png">

Receiver Operating Characteristic (ROC) curves and the AUC metric of the ROC curve.
<img src = "/assets/images/milestone2/q32_logR_ROC_angle.png">

### Q3.3

<div class="message">
Now train two more Logistic Regression classifiers using the same setup as above, but this time on the angle feature, and then both distance and angle. Produce the same three curves as described in the previous section for each model. Including the random baseline, you should have a total of 4 lines on each figure: 
Logistic Regression, trained on distance only (already done above)
Logistic Regression, trained on angle only
Logistic Regression, trained on both distance and angle
Random baseline: predicted probability is sampled from a uniform distribution.
Include these four figures (each with four curves) in your blog post. In a few sentences, discuss your interpretation of these results.
</div>

<img src = "/assets/images/milestone2/q33_logR_CP.png">
<img src = "/assets/images/milestone2/q33_logR_GR.png">
<img src = "/assets/images/milestone2/q33_logR_ROC_angle_distance.png">



### Q3.4

<div class="message">
Q3.4 Next to the figures (above), include links to the three experiment entries in your comet.ml projects that produced these three models. Save the three models to the three experiments on comet.ml (example here) and register them with some informative tags, as you will need it for the final section. \
</div>

Logistic Regression, trained on distance only comet.ml link - 

https://www.comet.ml/binulal/milestone-2/4d23e3c07b2c4130a7f6dbb50b5eb1e7


Logistic Regression, trained on angle only

<img src = "/assets/images/milestone2/q34_Random_CP.png">
<img src = "/assets/images/milestone2/q34_Random_CP.png">
<img src = "/assets/images/milestone2/q34_Random_CP.png">


## Feature Engineering II

### Q4.1
<div class="message">
 Upload the filtered DataFrame with all of the features that you created as a CSV using the log_dataframe_profile(...) method; keep the name as 'wpg_v_wsh_2017021065'.
</div>

The dataframe can be found in this [link to dataframe](https://www.comet.ml/binulal/milestone-2/b95f15e394374ac0b4509d170efe357d?assetId=995878f6b20d469d8fc8d2eb65ebd2f8&assetPath=dataframes&experiment-tab=assets).

### Q4.2
<div class="message">
In your blog post, add a list of all of the features that you created for this section. List each feature by both the column names in your data frame AND a simple human-readable explanation (i.e., game_sec: Total number of seconds elapsed in the game). If you created any novel features, briefly describe what they are. Add a link to the experiment which stores the filtered DataFrame artifact for the specified game. Note that there should only be one DataFrame logged with this name.
</div>

- Feature list (before preprocessing)
- Game_id : id for the hockey game
- Event_idx: id for an event (shot or goal) in the game
- Speed: (distance from the previous event)/(time since previous event seconds)
- periodSeconds_last: Time from the last event (seconds)
- eventType_last: Last event type
- Rebound: True only if the last event was also a shot
- Period: period in the hockey game
- periodType: ’REGULAR’,‘OVERTIME’,’SHOOTOUT’
- periodTime: min:sec, Time during the period
- periodSeconds: periodTime converted to seconds
- teamInfo: name of team who made the shot
- isGoal: True only if shot is goal
- shotType: different shot types
- Coordinates_x: x coordinate where the event happened
- Coordinates_y: y coordinate where the event happened
- Coordinates_x_last: x coordinate where the last event happened
- Coordinates_y_last: y coordinate where the last event happened
- Distance_last: distance between current and last event
- Dist_goal: distance between event and goal
- Angle_goal: angle [-180,180] between event and goal
- Angle_change: only when shot is a rebound, the change in angle between shots
- Angle_speed: Angle_change/(time between current and last shot)
Shooter: name of shooter
- Goalie: name of goalie
- emptyNet: True if a shot is taken on an empty Net.
- Strength: Strength of team on the playing field
- homeTeam: name of home team
- awayTeam: name of away team
- homeSide: Left if home starts in period 1 on the left side, right otherwise.



## Advanced Models

### Q5.1

<div class="message">
Add the corresponding curves to the four figures in your blog post. Briefly (few sentences) discuss your training/validation setup, and compare the results to the Logistic Regression baseline. Include a link to the relevant comet.ml entry for this experiment, but you do not need to log this model to the model registry.
</div>

<img src="/assets/images/milestone2/q51_XGboost_CP.png">

<img src="/assets/images/milestone2/q51_XGboost_GR.png">

<img src="/assets/images/milestone2/q51_Xgboost_ROC_distance_angle.png">

### Q5.2

<div class="message">
Now, train an XGBoost classifier using all of the features you created in Part 4 and do some hyperparameter tuning to try to find the best performing model with all of these features. In your blog post, discuss your hyperparameter tuning setup, and include figures to substantiate your choice of hyperparameters. For example, you could select appropriate metrics and do a grid search with cross validation. Once, include curves corresponding to the best model to the four figures in your blog post, and briefly compare the results to the XGBoost baseline. Include a link to the relevant comet.ml entry for this experiment, and log this model to the model registry.
</div>

<img src="/assets/images/milestone2/q52_XGboost_CP.png">

<img src="/assets/images/milestone2/q52_XGboost_GR.png">

<img src="/assets/images/milestone2/q52_Xgboost_ROC.png">


### Q5.3

<div class="message">
Finally, explore using some feature selection techniques to see if you can simplify your input features. A number of features carry correlated information, so you can try to see if some of them are redundant. You can try some of the feature selection techniques discussed in class; many of these are implemented for you by scikit-learn.. You could also use a library like SHAP to try to interpret what feature your model relies on the most. Discuss the feature selection strategies that you tried, and what was the most optimal set of features that you came up with. Include some figures to substantiate your claims. Once you’ve found the optimal set of features via hyperparameter tuning/cross validation, if the feature set is different than what was used for Q2 of this section, include curves corresponding to the best model to the four figures in your blog post, and briefly compare the results to the XGBoost baseline. Include a link to the relevant comet.ml entry for this experiment, and log this model to the model registry.
</div>

## Give it your best shot

### Q6.1
<div class="message">
In your blog post, discuss the various techniques and methods you tried. Include the same four figures as in Part 3 (ROC/AUC curve, goal rate vs. probability percentile, cumulative proportion of goals vs. probability percentile, and the reliability curve). Quantitative metrics are only required for a few sets of experiments, so you only need to include a few curves on each plot (e.g., things that you found interesting, or models that performed particularly well). Make sure to include and highlight what you consider your best ‘final’ model. For methods that weren’t too successful or interesting, you can just include them as short qualitative discussion.
</div>

For coming up with the best model, we 

- made sure to preprocess the data into a numerical form. -
- Redundant columns are dropped such as ‘game_id’, ‘event_idx’, ‘periodTime’. 
- All columns with more than 60% NAN values are removed. - 
- All rows with NAN values are also dropped. 
- ‘isGoal’, ‘rebound’, ‘emptyNet’, ‘homeSide’ columns are binarized. 
- One hot encoding is applied to the columns ‘periodType’, ‘eventType_last’, ‘teamInfo’, ‘shotType’, ‘homeTeam’, ‘awayTeam’. 
- ‘shooter’ and ‘goalie’ columns are dropped because of the low data variance. The total number of features ended up being 128.

After preprocessing, the data was **stratified-split** into train validation sets. 

For the **logistic regression model**, a **grid search** cross validation strategy was used to select the *C* parameter and the *number of components* after fitting the data with PCA. The data is first standardized in the pipeline, then PCA is applied then finally the logistic regression model. The resulting *best* parameters were PCA_n=85, C=0.0001.

For the **svc model**, a **grid search** cross validation strategy was used to select the *C, kernel* parameters. The data is first standardized in the pipeline, then PCA (with number of components = 85) is applied then finally the svc model. The resulting best paracters were C=10, kernel=’rbf’.

<img src="/assets/images/milestone2/q61_1.png">


For the **random forest model**, The data is directly applied to the random forest model with a predefined parameter. The random forest model has an **AUC of 0.77**, highest among all models.

For the **adaboost model**, a grid search cross validation strategy was used to select the *n_estimators*, *learning_rate* parameters. The data is directly applied to the adaboost model. The *resulting* best parameters were n_estimators=10, learning_rate=0.01.

<img src="/assets/images/milestone2/q61_2.png">

For the **mlp model**, a **grid search** cross validation strategy was used to select the *hidden_layer_sizes*. The data is first standardized in the pipeline, then PCA is applied then finally the mlp model. The resulting best paracters were C=10, kernel=’rbf’. 
The MLP model seems to have a smooth curve on the shot prob model percentile plots and the **best fitting reliability diagram**. It has an **AUC of 0.74**

<img src="/assets/images/milestone2/q61_3.png">

Finally For the **voting ensemble model**, we have included the previous random forest, mlp, logistic regression and adaboost models with *equal* weights. The data is fitted with *soft voting*. The final model used on the test set was the ensemble model in hopes for a better generalization performance. It has an **AUC of 0.76**.

The **precision** metric for most of the models is *close to 1, other than svc (0.58)*. The **f1 score** is *around 0.1* for most of the models. The **recall score** is around *0.06* for most of the models. These scores seem to be a lot lower because of the false negatives. It seems to be hard for the models to learn which shots lead to a goal.


### Q6.2

<div class="message">
Next to the figures, include links to the experiment entry in your comet.ml projects that you included quantitative metrics for (around 3-4). Log the models to the experiments on comet.ml (example here) and register them with some informative tags.
</div>

Experiments
Q61_logR: https://www.comet.ml/binulal/milestone-2/3c948faf0aab4f65a2ec6e90f4257934

Q62_svc: (didn’t use in ensemble) poor precision https://www.comet.ml/binulal/milestone-2/e9047a02b9df4f87a8c755e8731f0c60

Q63_rf: https://www.comet.ml/binulal/milestone-2/b1012869675c4a5bb3c82af6e7ddbced

Q64_ada: https://www.comet.ml/binulal/milestone-2/935ac0dc5741452dbca213730d59ef19

Q65_nn: https://www.comet.ml/binulal/milestone-2/5af8669662854d01915010bd9436c9d0

Q66_ens: https://www.comet.ml/binulal/milestone-2/74aa344baea9429abb9541451d18f53d



## Evaluate on test set

### Q7.1

<div class="message">
Test your 5 models on the untouched 2019/20 regular season dataset. In your blog post, include the four figures described above. Discuss your results and observations on the test set. Do your models perform as well on the test set as you did on your validation set when building your models? Do any models perform better or worse than you expected?
</div>


### Q7.2

<div class="message">
Test your 5 models on the untouched 2019/20 playoff games. In your blog post, include the four figures described above. Discuss your results and observations on this test set. Are there any differences to the regular-season test set or do you get similar ‘generalization’ performance?
</div>

<img src="/assets/images/milestone2/q7m5_ens_CP.png">
<img src="/assets/images/milestone2/q7m5_ens_GR.png">
<img src="/assets/images/milestone2/q7m5_ens_RD.png">
<img src="/assets/images/milestone2/q7m5_ens_ROC.png">