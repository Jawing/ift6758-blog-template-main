---
layout: post
title: Milestone 2
---

# Milestone 2 Presented by [this notebook](https://github.com/Jawing/ift6758-project-template-main/blob/main/milestone2.ipynb)

## Experiment Tracking

## Feature Engineering I

### Q2.1 

<div class="message">
Create and include the following figures in your blog post and briefly discuss your observations (few sentences). As always, make sure all of your axes are labeled correctly, and you make the appropriate choice of axis scale.:
 
</div>

- A histogram of shot counts (goals and no-goals separated), binned by *angle*

Although teams like to shot at -30, 0 (meaning shooting directly in front of the net) and 30 degrees, the goals are distributed as bell-curve shape with 0 degree shots having the highest probability to score. 

<img src = "/assets/images/milestone2/m2_q2_1_angle_to_goal.png">

- A histogram of shot counts (goals and no-goals separated), binned by *distance*

Scoring is harder when shot distance is larger than 25 feet, yet teams shots are quite evenly distributed across different distances.

<img src = "/assets/images/milestone2/m2_q2_1_dist_to_goal.png">

- A 2D histogram where one axis is the distance and the other is the angle. You do not need to separate goals and no-goals.
Hint: check out joint plots.

It seems that shots have a very hard time turning into goals when shot angles are larger than 50 or when distance larger than 40.

<img src = "/assets/images/milestone2/m2_q2_1_joint_ang_to_dist.png">

Smoothed by kde:
<img src = "/assets/images/milestone2/m2_q2_1_joint_ang_to_dist_kde.png">



### Q2.2

<div class="message">
Now, create two more figures relating the goal rate, i.e., #goals / (#no_goals + #goals), to the distance, and goal rate to the angle of the shot. Include these figures in your blog post and briefly discuss your observations.
</div>

The goal ratio is highest within 20 feet, lower when between 20-40 feet and much lower outside of 40 feet.

Distance:
<img src = "/assets/images/milestone2/m2_q2_2_shot_dist_goal_ratio.png">

The goal ratio is a bell-shape curve with highest point at around zero.

Shot angle:
<img src = "/assets/images/milestone2/m2_q2_2_shot_angle_goal_ratio.png">

### Q2.3

<div class="message">
We can use our “domain knowledge” for some quick sanity checks! The domain knowledge is that “it is incredibly rare to score a non-empty net goal on the opposing team from within your defensive zone”. Knowing this, create another histogram, this time of goals only, binned by distance, and separate empty net and non-empty net events. Include this figure in your blog post and discuss your observations. Can you find any events that are incorrectly labeled? If yes, prove that one event is incorrectly labeled.
Hint: the NHL game center usually has video clips of goals for every game.
</div>

<img src = "/assets/images/milestone2/m2_q2_3_dist_to_emptynet.png">

## Baseline Models

### Q3.1

Using your dataset (remember, don’t touch the test set!), create a training and validation split, however, you feel is appropriate. Using only the distance feature, train a Logistic Regression classifier with the completely default settings, i.e.: 

clf = LogisticRegression()
clf.fit(X, y)

Evaluate the accuracy (i.e., correctly predicted / total) of your model on the validation set. What do you notice? Look at the predictions and discuss your findings. What could be a potential issue? Include these discussions in your blog post.

### Q3.2

Based on your findings in Q1, we should explore other ways of evaluating our model. The first thing to note is that we are not actually interested in the binary prediction of whether a shot is a goal or not - we are interested in the probability that the model assigns to it (recall that we’re interested in the notion of expected goals). Scikit-learn provides this to you via the predict_proba(...) method; make sure you take the probabilities of the class that you care about! You will produce four figures (one curve per model per plot) to probe our model’s performance. Make sure you are using the probabilities obtained on the validation set:
Receiver Operating Characteristic (ROC) curves and the AUC metric of the ROC curve. Include a random classifier baseline, i.e., each shot has a 50% chance of being a goal.
The goal rate (#goals / (#no_goals + #goals)) as a function of the shot probability model percentile, i.e., if a value is the 70th percentile, it is above 70% of the data. 
The cumulative proportion of goals (not shots) as a function of the shot probability model percentile.
The reliability diagram (calibration curve). Scikit-learn provides functionality to create a reliability diagram in a few lines of code; check out the CalibrationDisplay API (specifically the .from_estimator() or .from_predictions() methods) for more information.

### Q3.3

Now train two more Logistic Regression classifiers using the same setup as above, but this time on the angle feature, and then both distance and angle. Produce the same three curves as described in the previous section for each model. Including the random baseline, you should have a total of 4 lines on each figure: 
Logistic Regression, trained on distance only (already done above)
Logistic Regression, trained on angle only
Logistic Regression, trained on both distance and angle
Random baseline: predicted probability is sampled from a uniform distribution, i.e., yiU(0,1)
Include these four figures (each with four curves) in your blog post. In a few sentences, discuss your interpretation of these results.

### Q3.4

Next to the figures (above), include links to the three experiment entries in your comet.ml projects that produced these three models. Save the three models to the three experiments on comet.ml (example here) and register them with some informative tags, as you will need it for the final section. \

## Feature Engineering II

In your blog post, add a list of all of the features that you created for this section. List each feature by both the column names in your data frame AND a simple human-readable explanation (i.e., game_sec: Total number of seconds elapsed in the game). If you created any novel features, briefly describe what they are. Add a link to the experiment which stores the filtered DataFrame artifact for the specified game. Note that there should only be one DataFrame logged with this name.

## Advanced Models

### Q5.1

Add the corresponding curves to the four figures in your blog post. Briefly (few sentences) discuss your training/validation setup, and compare the results to the Logistic Regression baseline. Include a link to the relevant comet.ml entry for this experiment, but you do not need to log this model to the model registry.

### Q5.2

Now, train an XGBoost classifier using all of the features you created in Part 4 and do some hyperparameter tuning to try to find the best performing model with all of these features. In your blog post, discuss your hyperparameter tuning setup, and include figures to substantiate your choice of hyperparameters. For example, you could select appropriate metrics and do a grid search with cross validation. Once, include curves corresponding to the best model to the four figures in your blog post, and briefly compare the results to the XGBoost baseline. Include a link to the relevant comet.ml entry for this experiment, and log this model to the model registry.

### Q5.3

Finally, explore using some feature selection techniques to see if you can simplify your input features. A number of features carry correlated information, so you can try to see if some of them are redundant. You can try some of the feature selection techniques discussed in class; many of these are implemented for you by scikit-learn.. You could also use a library like SHAP to try to interpret what feature your model relies on the most. Discuss the feature selection strategies that you tried, and what was the most optimal set of features that you came up with. Include some figures to substantiate your claims. Once you’ve found the optimal set of features via hyperparameter tuning/cross validation, if the feature set is different than what was used for Q2 of this section, include curves corresponding to the best model to the four figures in your blog post, and briefly compare the results to the XGBoost baseline. Include a link to the relevant comet.ml entry for this experiment, and log this model to the model registry.

## Give it your best shot

### Q6.1

In your blog post, discuss the various techniques and methods you tried. Include the same four figures as in Part 3 (ROC/AUC curve, goal rate vs. probability percentile, cumulative proportion of goals vs. probability percentile, and the reliability curve). Quantitative metrics are only required for a few sets of experiments, so you only need to include a few curves on each plot (e.g., things that you found interesting, or models that performed particularly well). Make sure to include and highlight what you consider your best ‘final’ model. For methods that weren’t too successful or interesting, you can just include them as short qualitative discussion.

### Q6.2

Next to the figures, include links to the experiment entry in your comet.ml projects that you included quantitative metrics for (around 3-4). Log the models to the experiments on comet.ml (example here) and register them with some informative tags.

## Evaluate on test set

### Q7.1

Test your 5 models on the untouched 2019/20 regular season dataset. In your blog post, include the four figures described above. Discuss your results and observations on the test set. Do your models perform as well on the test set as you did on your validation set when building your models? Do any models perform better or worse than you expected?

### Q7.2

Test your 5 models on the untouched 2019/20 playoff games. In your blog post, include the four figures described above. Discuss your results and observations on this test set. Are there any differences to the regular-season test set or do you get similar ‘generalization’ performance?
